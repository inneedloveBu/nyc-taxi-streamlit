"""
é«˜çº§Sparkå¤„ç†å™¨ - åŸºäºä½ NLPé¡¹ç›®çš„ç»éªŒ
"""
import sys
import time
from pathlib import Path
from datetime import datetime
import pandas as pd
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from src.path_utils import get_data_path, get_project_root
import findspark
findspark.init()

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql.window import Window
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.clustering import KMeans

class AdvancedNYCDataProcessor:
    def __init__(self, app_name="NYCTaxiAdvancedProcessor", master="local[*]"):
        """åˆå§‹åŒ–Sparkä¼šè¯ - å€Ÿé‰´ä½ NLPé¡¹ç›®çš„é…ç½®"""
        self.start_time = time.time()
        self.project_root = get_project_root()
        self.output_dir = self.project_root / "output" / "spark_advanced"
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºSparkä¼šè¯ï¼ˆä½¿ç”¨ä½ ç†Ÿæ‚‰çš„é…ç½®æ–¹å¼ï¼‰
        self.spark = SparkSession.builder \
            .appName(app_name) \
            .master(master) \
            .config("spark.executor.memory", "2g") \
            .config("spark.driver.memory", "2g") \
            .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
            .config("spark.sql.repl.eagerEval.enabled", "true") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .config("spark.ui.port", "4040") \
            .config("spark.logConf", "true") \
            .getOrCreate()
        
        # è®¾ç½®æ—¥å¿—çº§åˆ«
        self.spark.sparkContext.setLogLevel("WARN")
        print(f"âœ… Sparkä¼šè¯å·²åˆ›å»º: {app_name}")
        
    def load_and_validate_data(self, file_pattern="*.parquet"):
        """åŠ è½½å¹¶éªŒè¯æ•°æ®"""
        print("ğŸ“‚ åŠ è½½æ•°æ®...")
        
        data_dir = self.project_root / "data" / "raw"
        
        # æŸ¥æ‰¾æ‰€æœ‰æ•°æ®æ–‡ä»¶
        data_files = list(data_dir.glob(file_pattern))
        
        if not data_files:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼Œåˆ›å»ºç¤ºä¾‹æ•°æ®
            print("âš ï¸  æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼Œåˆ›å»ºç¤ºä¾‹æ•°æ®...")
            df = self._create_sample_spark_data()
            # ä¿å­˜ä¸ºParquet
            sample_path = data_dir / "yellow_tripdata_sample.parquet"
            df.write.parquet(str(sample_path), mode="overwrite")
            return df
        else:
            # åŠ è½½ç¬¬ä¸€ä¸ªæ–‡ä»¶ï¼ˆæˆ–å¤šä¸ªæ–‡ä»¶ï¼‰
            file_path = data_files[0]
            print(f"ğŸ“„ åŠ è½½æ–‡ä»¶: {file_path.name}")
            
            if file_path.suffix.lower() == '.parquet':
                df = self.spark.read.parquet(str(file_path))
            elif file_path.suffix.lower() == '.csv':
                df = self.spark.read.csv(str(file_path), header=True, inferSchema=True)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path.suffix}")
        
        # æ•°æ®éªŒè¯
        print("ğŸ” æ•°æ®éªŒè¯...")
        print(f"  æ•°æ®å½¢çŠ¶: {df.count():,} è¡Œ Ã— {len(df.columns)} åˆ—")
        print(f"  åˆ—å: {', '.join(df.columns[:10])}{'...' if len(df.columns) > 10 else ''}")
        df.printSchema()
        
        # æ˜¾ç¤ºæ•°æ®æ ·æœ¬
        print("ğŸ“‹ æ•°æ®æ ·æœ¬:")
        df.show(5, truncate=False)
        
        return df
    
    def _create_sample_spark_data(self, n_rows=10000):
        """åˆ›å»ºSparkç¤ºä¾‹æ•°æ®ï¼ˆå½“æ²¡æœ‰çœŸå®æ•°æ®æ—¶ï¼‰"""
        print("ğŸ² åˆ›å»ºSparkç¤ºä¾‹æ•°æ®...")
        
        from pyspark.sql.types import StructType, StructField
        from pyspark.sql.types import IntegerType, DoubleType, TimestampType, StringType
        
        schema = StructType([
            StructField("VendorID", IntegerType(), True),
            StructField("tpep_pickup_datetime", TimestampType(), True),
            StructField("tpep_dropoff_datetime", TimestampType(), True),
            StructField("passenger_count", IntegerType(), True),
            StructField("trip_distance", DoubleType(), True),
            StructField("PULocationID", IntegerType(), True),
            StructField("DOLocationID", IntegerType(), True),
            StructField("RatecodeID", IntegerType(), True),
            StructField("store_and_fwd_flag", StringType(), True),
            StructField("payment_type", IntegerType(), True),
            StructField("fare_amount", DoubleType(), True),
            StructField("extra", DoubleType(), True),
            StructField("mta_tax", DoubleType(), True),
            StructField("tip_amount", DoubleType(), True),
            StructField("tolls_amount", DoubleType(), True),
            StructField("improvement_surcharge", DoubleType(), True),
            StructField("total_amount", DoubleType(), True),
            StructField("congestion_surcharge", DoubleType(), True),
        ])
        
        # åˆ›å»ºç¤ºä¾‹æ•°æ®
        np.random.seed(42)
        
        # ç”Ÿæˆæ•°æ®
        data = []
        base_time = datetime(2023, 1, 1)
        
        for i in range(n_rows):
            pickup_time = base_time + pd.Timedelta(minutes=np.random.randint(0, 30*24*60))
            dropoff_time = pickup_time + pd.Timedelta(minutes=np.random.randint(5, 60))
            
            trip_distance = np.random.exponential(3)  # å¤§å¤šæ•°è¡Œç¨‹è¾ƒçŸ­
            if trip_distance > 50:  # é™åˆ¶å¼‚å¸¸å€¼
                trip_distance = 50
                
            fare_amount = 2.5 + trip_distance * 2.5 + np.random.randn() * 3
            if fare_amount < 2.5:
                fare_amount = 2.5
                
            tip_amount = fare_amount * np.random.choice([0, 0.1, 0.15, 0.2], p=[0.2, 0.3, 0.4, 0.1])
            
            row = (
                1,  # VendorID
                pickup_time,  # tpep_pickup_datetime
                dropoff_time,  # tpep_dropoff_datetime
                np.random.randint(1, 6),  # passenger_count
                round(trip_distance, 2),  # trip_distance
                np.random.randint(1, 264),  # PULocationID
                np.random.randint(1, 264),  # DOLocationID
                np.random.choice([1, 2, 3, 4, 5, 6]),  # RatecodeID
                "N",  # store_and_fwd_flag
                np.random.choice([1, 2, 3, 4, 5, 6]),  # payment_type
                round(fare_amount, 2),  # fare_amount
                round(np.random.choice([0, 0.5, 1.0]), 2),  # extra
                0.5,  # mta_tax
                round(tip_amount, 2),  # tip_amount
                round(np.random.choice([0, 1.25, 4.5, 5.76]), 2),  # tolls_amount
                0.3,  # improvement_surcharge
                round(fare_amount + tip_amount + 0.5 + 0.3, 2),  # total_amount
                round(np.random.choice([0, 2.5]), 2),  # congestion_surcharge
            )
            data.append(row)
        
        df = self.spark.createDataFrame(data, schema=schema)
        print(f"âœ… å·²åˆ›å»º {n_rows:,} è¡Œç¤ºä¾‹æ•°æ®")
        return df
    
    def preprocess_data(self, df):
        """æ•°æ®é¢„å¤„ç†"""
        print("ğŸ§¹ æ•°æ®é¢„å¤„ç†...")
        
        initial_count = df.count()
        
        # 1. åŸºæœ¬æ¸…æ´—
        df_clean = df.filter(
            (col("PULocationID").isNotNull()) &
            (col("DOLocationID").isNotNull()) &
            (col("tpep_pickup_datetime").isNotNull()) &
            (col("tpep_dropoff_datetime").isNotNull()) &
            (col("total_amount") > 0) &
            (col("total_amount") < 1000) &
            (col("trip_distance") > 0) &
            (col("trip_distance") < 100) &
            (col("passenger_count") > 0) &
            (col("passenger_count") <= 6)
        )
        
        # 2. æ·»åŠ æ—¶é—´ç‰¹å¾
        df_clean = df_clean.withColumn("pickup_hour", hour(col("tpep_pickup_datetime"))) \
                          .withColumn("pickup_day", dayofmonth(col("tpep_pickup_datetime"))) \
                          .withColumn("pickup_dayofweek", dayofweek(col("tpep_pickup_datetime"))) \
                          .withColumn("pickup_month", month(col("tpep_pickup_datetime"))) \
                          .withColumn("trip_duration_minutes", 
                                     (unix_timestamp(col("tpep_dropoff_datetime")) - 
                                      unix_timestamp(col("tpep_pickup_datetime"))) / 60)
        
        # 3. è®¡ç®—è¡ç”Ÿç‰¹å¾
        df_clean = df_clean.withColumn("speed_mph", 
                                      when(col("trip_duration_minutes") > 0,
                                           col("trip_distance") / (col("trip_duration_minutes") / 60))
                                      .otherwise(0))
        
        df_clean = df_clean.withColumn("tip_percentage",
                                      when(col("fare_amount") > 0,
                                           (col("tip_amount") / col("fare_amount")) * 100)
                                      .otherwise(0))
        
        # 4. ç§»é™¤å¼‚å¸¸å€¼
        df_clean = df_clean.filter(
            (col("trip_duration_minutes") > 0) &
            (col("trip_duration_minutes") < 180) &  # 3å°æ—¶ä»¥å†…
            (col("speed_mph") < 100) &  # åˆç†é€Ÿåº¦
            (col("tip_percentage") < 100)  # å°è´¹ä¸è¶…è¿‡è½¦è´¹
        )
        
        cleaned_count = df_clean.count()
        removed_percent = ((initial_count - cleaned_count) / initial_count * 100) if initial_count > 0 else 0
        
        print(f"  æ¸…æ´—å‰: {initial_count:,} è¡Œ")
        print(f"  æ¸…æ´—å: {cleaned_count:,} è¡Œ")
        print(f"  ç§»é™¤: {initial_count - cleaned_count:,} è¡Œ ({removed_percent:.2f}%)")
        
        return df_clean
    
    def analyze_basic_metrics(self, df):
        """åŸºç¡€æŒ‡æ ‡åˆ†æ"""
        print("ğŸ“Š åŸºç¡€æŒ‡æ ‡åˆ†æ...")
        
        # 1. çƒ­é—¨è·¯çº¿ï¼ˆå‰100ï¼‰
        hot_routes = df.groupBy("PULocationID", "DOLocationID") \
                      .agg(
                          count("*").alias("trip_count"),
                          avg("trip_distance").alias("avg_distance"),
                          avg("total_amount").alias("avg_fare"),
                          avg("trip_duration_minutes").alias("avg_duration"),
                          avg("tip_amount").alias("avg_tip"),
                          stddev("total_amount").alias("fare_std")
                      ) \
                      .filter(col("trip_count") > 5) \
                      .orderBy(desc("trip_count")) \
                      .limit(100)
        
        # 2. åŒºåŸŸçƒ­åº¦åˆ†æ
        pickup_hotspots = df.groupBy("PULocationID") \
                           .agg(
                               count("*").alias("pickup_count"),
                               avg("total_amount").alias("avg_fare"),
                               avg("trip_distance").alias("avg_distance"),
                               avg("trip_duration_minutes").alias("avg_duration")
                           ) \
                           .orderBy(desc("pickup_count")) \
                           .limit(50)
        
        dropoff_hotspots = df.groupBy("DOLocationID") \
                            .agg(
                                count("*").alias("dropoff_count"),
                                avg("total_amount").alias("avg_fare")
                            ) \
                            .orderBy(desc("dropoff_count")) \
                            .limit(50)
        
        # 3. æ—¶é—´åˆ†æ
        hourly_traffic = df.groupBy("pickup_hour") \
                          .agg(
                              count("*").alias("trip_count"),
                              avg("total_amount").alias("avg_fare"),
                              avg("trip_distance").alias("avg_distance"),
                              avg("tip_percentage").alias("avg_tip_percentage")
                          ) \
                          .orderBy("pickup_hour")
        
        # 4. æ˜ŸæœŸåˆ†æ
        daily_traffic = df.groupBy("pickup_dayofweek") \
                         .agg(
                             count("*").alias("trip_count"),
                             avg("total_amount").alias("avg_fare"),
                             avg("tip_amount").alias("avg_tip")
                         ) \
                         .orderBy("pickup_dayofweek")
        
        # 5. ä¹˜å®¢æ•°é‡åˆ†æ
        passenger_stats = df.groupBy("passenger_count") \
                           .agg(
                               count("*").alias("trip_count"),
                               avg("total_amount").alias("avg_fare"),
                               avg("trip_distance").alias("avg_distance")
                           ) \
                           .filter(col("passenger_count").isNotNull()) \
                           .orderBy("passenger_count")
        
        return {
            "hot_routes": hot_routes,
            "pickup_hotspots": pickup_hotspots,
            "dropoff_hotspots": dropoff_hotspots,
            "hourly_traffic": hourly_traffic,
            "daily_traffic": daily_traffic,
            "passenger_stats": passenger_stats
        }
    
    def analyze_advanced_metrics(self, df):
        """é«˜çº§åˆ†æï¼ˆèšç±»ç­‰ï¼‰"""
        print("ğŸ”¬ é«˜çº§åˆ†æ...")
        
        try:
            # 1. è´¹ç”¨èšç±»åˆ†æ
            fare_features = df.select("trip_distance", "trip_duration_minutes", "total_amount")
            
            # åˆ›å»ºç‰¹å¾å‘é‡
            assembler = VectorAssembler(
                inputCols=["trip_distance", "trip_duration_minutes", "total_amount"],
                outputCol="features"
            )
            
            fare_features_vector = assembler.transform(fare_features).select("features")
            
            # æ ‡å‡†åŒ–
            scaler = StandardScaler(
                inputCol="features",
                outputCol="scaled_features",
                withStd=True,
                withMean=True
            )
            
            scaler_model = scaler.fit(fare_features_vector)
            scaled_data = scaler_model.transform(fare_features_vector)
            
            # KMeansèšç±»
            kmeans = KMeans(k=3, seed=42, featuresCol="scaled_features")
            kmeans_model = kmeans.fit(scaled_data)
            
            # é¢„æµ‹èšç±»
            clustered = kmeans_model.transform(scaled_data)
            
            # å°†èšç±»ç»“æœæ·»åŠ å›åŸå§‹æ•°æ®
            df_with_cluster = df.withColumn("row_idx", monotonically_increasing_id())
            clustered_with_idx = clustered.withColumn("row_idx", monotonically_increasing_id())
            
            df_clustered = df_with_cluster.join(clustered_with_idx.select("row_idx", "prediction"), 
                                               on="row_idx").drop("row_idx")
            
            # èšç±»ç»Ÿè®¡
            cluster_stats = df_clustered.groupBy("prediction") \
                                       .agg(
                                           count("*").alias("trip_count"),
                                           avg("trip_distance").alias("avg_distance"),
                                           avg("total_amount").alias("avg_fare"),
                                           avg("trip_duration_minutes").alias("avg_duration")
                                       ) \
                                       .orderBy("prediction")
            
            # 2. è®¡ç®—è¡Œç¨‹æ•ˆç‡æŒ‡æ ‡
            efficiency_stats = df.withColumn("fare_per_mile", 
                                           col("total_amount") / col("trip_distance")) \
                                .filter(col("fare_per_mile") > 0) \
                                .groupBy("pickup_hour") \
                                .agg(
                                    avg("fare_per_mile").alias("avg_fare_per_mile"),
                                    avg("speed_mph").alias("avg_speed"),
                                    count("*").alias("trip_count")
                                ) \
                                .orderBy("pickup_hour")
            
            return {
                "cluster_stats": cluster_stats,
                "efficiency_stats": efficiency_stats,
                "df_clustered": df_clustered
            }
            
        except Exception as e:
            print(f"âš ï¸  é«˜çº§åˆ†æå¤±è´¥: {e}")
            print("  ä½¿ç”¨åŸºç¡€åˆ†æä»£æ›¿...")
            return {}
    
    def save_results(self, basic_results, advanced_results=None):
        """ä¿å­˜åˆ†æç»“æœ"""
        print("ğŸ’¾ ä¿å­˜ç»“æœ...")
        
        # ä¿å­˜åŸºç¡€ç»“æœ
        for name, df in basic_results.items():
            # ä¿å­˜ä¸ºParquet
            parquet_path = self.output_dir / f"{name}.parquet"
            df.write.parquet(str(parquet_path), mode="overwrite")
            
            # ä¿å­˜ä¸ºCSVï¼ˆç”¨äºStreamlitï¼‰
            csv_path = self.output_dir / f"{name}.csv"
            pandas_df = df.toPandas()
            pandas_df.to_csv(csv_path, index=False)
            
            print(f"  âœ… {name}: {len(pandas_df):,} è¡Œ -> {csv_path}")
        
        # ä¿å­˜é«˜çº§åˆ†æç»“æœ
        if advanced_results:
            for name, df in advanced_results.items():
                if name == "df_clustered":
                    # ä¿å­˜èšç±»æ•°æ®ï¼ˆæŠ½æ ·ï¼‰
                    sample_df = df.sample(0.1)  # 10%æ ·æœ¬
                    csv_path = self.output_dir / f"{name}_sample.csv"
                    sample_df.toPandas().to_csv(csv_path, index=False)
                    print(f"  âœ… {name}_sample: {sample_df.count():,} è¡Œ")
                elif isinstance(df, pd.DataFrame):
                    csv_path = self.output_dir / f"{name}.csv"
                    df.to_csv(csv_path, index=False)
                else:
                    csv_path = self.output_dir / f"{name}.csv"
                    df.toPandas().to_csv(csv_path, index=False)
        
        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        self._generate_summary_report(basic_results)
    
    def _generate_summary_report(self, results):
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
        print("ğŸ“ ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š...")
        
        report = {
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "processing_time_seconds": round(time.time() - self.start_time, 2),
            "datasets": {}
        }
        
        for name, df in results.items():
            count = df.count()
            report["datasets"][name] = {
                "row_count": count,
                "column_count": len(df.columns),
                "sample_data": df.limit(3).toPandas().to_dict(orient="records") if count > 0 else []
            }
        
        # ä¿å­˜æŠ¥å‘Šä¸ºJSON
        import json
        report_path = self.output_dir / "analysis_report.json"
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        print(f"  âœ… æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
        # æ‰“å°å…³é”®æŒ‡æ ‡
        print("\nğŸ“ˆ å…³é”®æŒ‡æ ‡:")
        if "hot_routes" in results:
            top_routes = results["hot_routes"].limit(3).toPandas()
            print(f"  æœ€çƒ­é—¨è·¯çº¿: {top_routes.iloc[0]['PULocationID']} -> {top_routes.iloc[0]['DOLocationID']} "
                  f"({top_routes.iloc[0]['trip_count']} æ¬¡è¡Œç¨‹)")
        
        if "hourly_traffic" in results:
            peak_hour = results["hourly_traffic"].orderBy(desc("trip_count")).first()
            if peak_hour:
                print(f"  é«˜å³°æ—¶æ®µ: {int(peak_hour['pickup_hour'])}:00 "
                      f"({peak_hour['trip_count']} æ¬¡è¡Œç¨‹)")
    
    def run(self, use_advanced=True):
        """è¿è¡Œå®Œæ•´æµç¨‹"""
        print("=" * 60)
        print("ğŸš€ NYC Taxi é«˜çº§æ•°æ®åˆ†ææµç¨‹")
        print("=" * 60)
        
        try:
            # 1. åŠ è½½æ•°æ®
            df_raw = self.load_and_validate_data()
            
            # 2. æ•°æ®é¢„å¤„ç†
            df_clean = self.preprocess_data(df_raw)
            
            # 3. åŸºç¡€åˆ†æ
            basic_results = self.analyze_basic_metrics(df_clean)
            
            # 4. é«˜çº§åˆ†æï¼ˆå¯é€‰ï¼‰
            advanced_results = None
            if use_advanced:
                advanced_results = self.analyze_advanced_metrics(df_clean)
            
            # 5. ä¿å­˜ç»“æœ
            self.save_results(basic_results, advanced_results)
            
            # 6. æ˜¾ç¤ºæ‰§è¡Œæ—¶é—´
            total_time = time.time() - self.start_time
            print(f"\nâœ… åˆ†æå®Œæˆï¼æ€»è€—æ—¶: {total_time:.2f} ç§’")
            print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {self.output_dir}")
            
            return basic_results, advanced_results
            
        except Exception as e:
            print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return None, None
        
        finally:
            # æ¸…ç†èµ„æº
            self.spark.stop()
            print("ğŸ”„ Sparkä¼šè¯å·²å…³é—­")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="NYC Taxi é«˜çº§æ•°æ®åˆ†æ")
    parser.add_argument("--simple", action="store_true", help="ä½¿ç”¨ç®€å•æ¨¡å¼ï¼ˆè·³è¿‡é«˜çº§åˆ†æï¼‰")
    parser.add_argument("--sample", action="store_true", help="ä½¿ç”¨æ ·æœ¬æ•°æ®")
    
    args = parser.parse_args()
    
    # è¿è¡Œå¤„ç†å™¨
    processor = AdvancedNYCDataProcessor()
    
    # æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦ä½¿ç”¨é«˜çº§åˆ†æ
    use_advanced = not args.simple
    
    print(f"ä½¿ç”¨{'é«˜çº§' if use_advanced else 'åŸºç¡€'}åˆ†ææ¨¡å¼")
    
    processor.run(use_advanced=use_advanced)

if __name__ == "__main__":
    main()